{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 660M (CNMeM is disabled, cuDNN 5105)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.greedy = True\n",
    "%config IPCompleter.merge_completions = True\n",
    "%config IPCompleter.limit_to__all__ = False\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.core import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHANNELS_NUM = 1\n",
    "IMAGE_WIDTH = 48\n",
    "IMAGE_HIGHT = 48\n",
    "CLASSES_NUM = 7\n",
    "EPOCHS_NUM = 80\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001     #if from epoch 0, start with learning rate = 0.001\n",
    "DATASET_SIZE = 35887\n",
    "emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "FILE_NAME = \"fer2013/fer2013.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    X_data = np.zeros((DATASET_SIZE, IMAGE_HIGHT*IMAGE_WIDTH))\n",
    "    Y_data = np.zeros((DATASET_SIZE,))\n",
    " \n",
    "    i = 0\n",
    "    with open(FILE_NAME, 'rb') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            X_data[i, :] = np.fromstring(row['pixels'], dtype=int, sep=' ')\n",
    "            Y_data[i] = row['emotion']\n",
    "            i = i + 1\n",
    "            \n",
    "    X_data = X_data.reshape((-1, CHANNELS_NUM, IMAGE_HIGHT, IMAGE_WIDTH))\n",
    "    #print X_data.shape\n",
    "    #print Y_data.shape\n",
    "    \n",
    "    #############################################\n",
    "    #mask = np.random.choice(DATASET_SIZE, int((4.0/5)*DATASET_SIZE),replace = False)     #Random numbers must be unique.\n",
    "   \n",
    "    #Save the training data mask\n",
    "    #output = open('TrainingDataMask.oo', 'wb')\n",
    "    #pickle.dump(mask, output)\n",
    "    #output.close()\n",
    "    \n",
    "    #Load the training data mask\n",
    "    output = open('TrainingDataMask.oo', 'rb')\n",
    "    mask = pickle.load(output)\n",
    "    #############################################\n",
    "    \n",
    "    X_train = X_data[mask, :, :, :] \n",
    "    Y_train = Y_data[mask]\n",
    "    X_val_tst = X_data[~mask, :, :, :] \n",
    "    Y_val_tst = Y_data[~mask]\n",
    "    \n",
    "    \n",
    "    #val_mask = np.random.choice(X_val_tst.shape[0], int((1.0/10)*DATASET_SIZE),replace = False)\n",
    "    \n",
    "    #############################################\n",
    "    #Save the validation data mask\n",
    "    #output = open('ValDataMask.oo', 'wb')\n",
    "    #pickle.dump(val_mask, output)\n",
    "    #output.close()\n",
    "    \n",
    "    #Load the validation data mask\n",
    "    output = open('ValDataMask.oo', 'rb')\n",
    "    val_mask = pickle.load(output)\n",
    "    #############################################\n",
    "    \n",
    "    X_valid = X_val_tst[val_mask, :, :, :]\n",
    "    X_test = X_val_tst[~val_mask, :, :, :]\n",
    "    Y_valid = Y_val_tst[val_mask]\n",
    "    Y_test = Y_val_tst[~val_mask]\n",
    "    \n",
    "    #print X_train.shape, X_valid.shape, X_test.shape\n",
    "    #print Y_train.shape, Y_valid.shape, Y_test.shape\n",
    "\n",
    "    return dict(\n",
    "        X_train=(X_train).astype('float32'),\n",
    "        y_train=np_utils.to_categorical(Y_train.astype('uint8')),\n",
    "        X_valid=(X_valid).astype('float32'),\n",
    "        y_valid=np_utils.to_categorical(Y_valid.astype('uint8')),\n",
    "        X_test=(X_test).astype('float32'),\n",
    "        y_test=np_utils.to_categorical(Y_test.astype('uint8')),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        output_dim=CLASSES_NUM,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BuildModel():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, init = 'he_normal', input_shape=(CHANNELS_NUM, IMAGE_WIDTH, IMAGE_HIGHT), border_mode='same', W_regularizer = l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Convolution2D(16, 3, 3, init = 'he_normal', input_shape=(CHANNELS_NUM, IMAGE_WIDTH, IMAGE_HIGHT), border_mode='same', W_regularizer = l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Convolution2D(32, 3, 3, init = 'he_normal', input_shape=(CHANNELS_NUM, IMAGE_WIDTH, IMAGE_HIGHT), border_mode='same', W_regularizer = l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.4))  \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, init = 'he_normal', W_regularizer = l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(128, init = 'he_normal', W_regularizer = l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(CLASSES_NUM, init = 'he_normal', W_regularizer = l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))  \n",
    "    # Compile model\n",
    "    adam = Adam(lr=LEARNING_RATE*0.7*0.7*0.7, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    #load the model from a saved file\n",
    "    model.load_weights(\"weights.best.hdf5\")\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 16, 48, 48)    160         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 16, 48, 48)    96          convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 16, 48, 48)    0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 16, 48, 48)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 16, 48, 48)    2320        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 16, 48, 48)    96          convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 16, 48, 48)    0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 16, 48, 48)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 32, 48, 48)    4640        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 32, 48, 48)    96          convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32, 48, 48)    0           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32, 48, 48)    0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 73728)         0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           18874624    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 256)           512         dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 256)           0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256)           0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           32896       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 128)           256         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 128)           0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 128)           0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 7)             903         dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 7)             14          dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 7)             0           batchnormalization_6[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 18916613\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = BuildModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #save the currently best weights.\n",
    "    filepath=\"weights.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    #generator = ImageDataGenerator(\n",
    "        # rotation_range=30,\n",
    "        # width_shift_range=0.2,\n",
    "        # height_shift_range=0.2,\n",
    "        #shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        #horizontal_flip=True,\n",
    "        #fill_mode='nearest')\n",
    "    \n",
    "    #history = model.fit_generator(generator.flow(data['X_train'], data['y_train'],batch_size=BATCH_SIZE),verbose = 1, samples_per_epoch=len(data['X_train']), callbacks=callbacks_list, validation_data=(data['X_valid'], data['y_valid']), nb_epoch=EPOCHS_NUM),\n",
    "    history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_valid'], data['y_valid']),callbacks = callbacks_list, verbose = 1,shuffle=True, nb_epoch=EPOCHS_NUM, batch_size=BATCH_SIZE)\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(data['X_test'], data['y_test'], verbose=1)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "    #Save the model.\n",
    "    model.save_weights(\"model1epoch80.h5\")\n",
    "\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the model.\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "print model.evaluate(data['X_test'], data['y_test'],verbose=1)\n",
    "print model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "for i in range(1,9):\n",
    "    plt.subplot(7,7,i)\n",
    "    im = np.array(scipy.misc.imresize(plt.imread('test_images/{}.png'.format(i)),(IMAGE_HIGHT, IMAGE_WIDTH)))\n",
    "    im1 = rgb2gray(im)\n",
    "    plt.imshow(im, cmap='gray', interpolation='none')\n",
    "    output_eval = model.predict(im1.reshape((1,1,IMAGE_HIGHT,IMAGE_WIDTH)), batch_size=BATCH_SIZE, verbose=1)\n",
    "    #print emotions[np.argmax(output_eval)]\n",
    "    #print np.argmax(output_eval) \n",
    "    #print 'Output: ',output_eval\n",
    "    plt.title(emotions[np.argmax(output_eval)], fontsize=10)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "for i in range(1,30):\n",
    "    plt.subplot(7,7,i)\n",
    "    offset = 50\n",
    "    im1 = data['X_test'][i+offset][0]\n",
    "    plt.imshow(im1, cmap='gray', interpolation='none')\n",
    "    output_eval = model.predict(data['X_test'][i+offset].reshape((1,1,IMAGE_HIGHT,IMAGE_WIDTH)), batch_size=BATCH_SIZE, verbose=1)\n",
    "    #print emotions[np.argmax(output_eval)]\n",
    "    #print np.argmax(output_eval) \n",
    "    #print 'Output: ',output_eval\n",
    "    #print 'Original: ',data['y_test'][i+offset]\n",
    "    plt.title(emotions[np.argmax(output_eval)], fontsize=10)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_faces(img):\n",
    "    rgbimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(rgbimg, cv2.COLOR_RGB2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.2,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags = cv2.cv.CV_HAAR_SCALE_IMAGE\n",
    "    )\n",
    "    #print(\"Found {0} faces!\".format(len(faces)))\n",
    "    faces_ = np.zeros((len(faces), 48, 48, 3))\n",
    "    for i, (x, y, w, h) in enumerate(faces):\n",
    "        cv2.rectangle(rgbimg, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        faces_[i] = imresize(rgbimg[y:y+h, x:x+w], (48, 48))\n",
    "    return rgbimg,faces_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(1,7):\n",
    "    img = cv2.imread('test_images/{}.jpg'.format(j))\n",
    "    rgbimg,faces = detect_faces(img)\n",
    "    plt.figure(figsize=(10,15))\n",
    "    for i in range(faces.shape[0]):\n",
    "        plt.subplot(7,7,i+1)\n",
    "        im = faces[i]\n",
    "        im = rgb2gray(im)\n",
    "        # print im.shape\n",
    "        # im = np.transpose(im, (2,0,1))\n",
    "        plt.imshow(im, cmap='gray', interpolation='none')\n",
    "        output_eval = model.predict(im.reshape((1,1,IMAGE_HIGHT,IMAGE_WIDTH)), batch_size=BATCH_SIZE, verbose=1)\n",
    "        #print emotions[np.argmax(output_eval)]\n",
    "        #print np.argmax(output_eval) \n",
    "        #print 'Output: ',output_eval\n",
    "        plt.title(emotions[np.argmax(output_eval)], fontsize=10)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Video Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    start_time = time.time()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    old = []\n",
    "    output_str=\"\"\n",
    "    #plt.figure(figsize=(10,15))\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, output = cap.read()\n",
    "\n",
    "        # Display the resulting frame\n",
    "        rgbimg,faces = detect_faces(output)\n",
    "        output = cv2.cvtColor(rgbimg, cv2.COLOR_RGB2BGR)\n",
    "       \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if elapsed_time >= 2:\n",
    "            for i in range(faces.shape[0]):\n",
    "                old = []\n",
    "                im = faces[i]\n",
    "                im = rgb2gray(im)\n",
    "                #plt.subplot(7,7,i+1)\n",
    "                #plt.imshow(im, cmap='gray', interpolation='none')\n",
    "                output_eval = model.predict(im.reshape((1,1,IMAGE_HIGHT,IMAGE_WIDTH)), batch_size=BATCH_SIZE, verbose=0)\n",
    "                old.append(emotions[np.argmax(output_eval)])\n",
    "            start_time = time.time()\n",
    "            #plt.show()\n",
    "            if(len(old)>0): \n",
    "                output_str = \"\"\n",
    "            \n",
    "            for i in range(len(old)):\n",
    "                output_str = output_str + \" \" + old[i]\n",
    "    \n",
    "        x = 10 #position of text\n",
    "        y = 20\n",
    "        cv2.putText(output, output_str, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0))\n",
    "\n",
    "        cv2.imshow('frame',output)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    # When everything done, release the capture\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
